nodes {
  kw_ast
  kw_nodes
  kw_tokenizer
  kw_rule
  kw_verbatim

  eq pipe star question dot
  lbrace rbrace
  langle rangle
  lparen rparen
  ident
  simple_string
  hash_string

  file
  string
  verbatim_def
  nodes_def
  tokenizer_def
  lex_rule
  syn_rule
  alt
  part

  ast_def
  ast_node_def
  method_def
  ast_selector
}

tokenizer {
  eq '='
  pipe '|'
  star '*'
  question '?'
  dot '.'
  lbrace '{'
  rbrace '}'
  langle '<'
  rangle '>'
  lparen '('
  rparen ')'
  kw_nodes 'nodes'
  kw_tokenizer 'tokenizer'
  kw_rule 'rule'
  kw_verbatim 'verbatim'
  kw_ast 'ast'

  whitespace r"\s+"
  simple_string r#"'([^'\\]|\\.)*'"#
  hash_string r"r#*" 'parse_raw_string'
  ident r"\w+"
}

rule file { nodes_def tokenizer_def <rep {syn_rule}> <opt {verbatim_def}> <opt {ast_def}> }

rule nodes_def {
  'nodes' <commit> '{' <rep {ident}> '}'
}

rule tokenizer_def {
  'tokenizer' <commit> '{' <rep {lex_rule}> '}'
}

rule lex_rule { ident <commit> string <opt {string}> }

rule syn_rule {
  'rule' <commit> ident syn_rule_body
}
rule syn_rule_body { '{' <opt {alt}> <rep {'|' alt}> '}' }

rule alt { <rep {part}> }

rule part { ident | simple_string | '<' ident <opt {syn_rule_body}> '>' }

rule string { simple_string | hash_string }

rule verbatim_def { 'verbatim' hash_string }

rule ast_def { 'ast' '{' <rep {ast_node_def}> '}' }

rule ast_node_def {
  ident '{' <rep {method_def}> '}'
}

rule method_def { ident ast_selector }
rule ast_selector { ident <opt {ast_selector_suffix}> }
rule ast_selector_suffix { '*' | '?' | '.' ident }


verbatim r#########"

fn parse_raw_string(s: &str) -> Option<usize> {
    let quote_start = s.find('"').unwrap();
    let q_hashes = concat!('"', "######", "######", "######", "######", "######");
    let closing = &q_hashes[..quote_start];
    s[quote_start + 1..].find(closing).map(|i| i + quote_start + 1 + closing.len())
}

"#########

ast {
  file {
    nodes_def nodes_def?
    tokenizer_def tokenizer_def?
    syn_rules syn_rule*
    verbatim_def verbatim_def?
    ast_def ast_def?
  }

  nodes_def { }

  tokenizer_def {
    lex_rules lex_rule*
  }

  lex_rule {
    node_type IDENT.text
  }

  syn_rule {
    name IDENT.text
    alts alt*
  }

  alt {
    parts part*
  }

  part { }

  verbatim_def { }

  ast_def {
    ast_nodes ast_node_def*
  }

  ast_node_def {
    name IDENT.text
    methods method_def*
  }

  method_def {
    name IDENT.text
  }
}
