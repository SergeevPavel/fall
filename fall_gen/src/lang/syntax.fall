nodes {
  kw_ast
  kw_nodes
  kw_node
  kw_class
  kw_tokenizer
  kw_rule
  kw_verbatim
  kw_pub

  eq pipe star question dot
  lbrace rbrace
  langle rangle
  lparen rparen
  ident
  simple_string
  hash_string

  file
  string
  verbatim_def
  nodes_def
  tokenizer_def
  lex_rule
  syn_rule

  ast_def
  ast_node_def
  ast_class_def
  method_def
  ast_selector

  ref_expr
  call_expr
  block_expr
  seq_expr
}

tokenizer {
  eq '='
  pipe '|'
  star '*'
  question '?'
  dot '.'
  lbrace '{'
  rbrace '}'
  langle '<'
  rangle '>'
  lparen '('
  rparen ')'
  kw_nodes 'nodes'
  kw_node 'node'
  kw_class 'class'
  kw_tokenizer 'tokenizer'
  kw_rule 'rule'
  kw_verbatim 'verbatim'
  kw_ast 'ast'
  kw_pub 'pub'

  whitespace r"\s+"
  simple_string r#"'([^'\\]|\\.)*'"#
  hash_string r"r#*" 'parse_raw_string'
  ident r"\w+"
}

pub rule file { nodes_def tokenizer_def <rep syn_rule> <opt verbatim_def> <opt ast_def> }

pub rule nodes_def {
  'nodes' <commit> '{' <rep ident {ident} {'}'}> '}'
}

pub rule tokenizer_def {
  'tokenizer' <commit> '{' <rep lex_rule> '}'
}

pub rule lex_rule { ident <commit> string <opt string> }

pub rule syn_rule {
  <opt 'pub'> 'rule' <commit> ident block_expr
}

rule string { simple_string | hash_string }

pub rule verbatim_def { 'verbatim' hash_string }

pub rule ast_def { 'ast' '{' <rep {ast_node_def | ast_class_def}> '}' }

pub rule ast_node_def {
  'node' ident '{' <rep method_def> '}'
}

pub rule ast_class_def {
  'class' ident '{' <rep ident> '}'
}

pub rule method_def { ident ast_selector }
pub rule ast_selector { ident <opt ast_selector_suffix> }
rule ast_selector_suffix { '*' | '?' | '.' ident }

rule expr { call_expr | ref_expr | block_expr }
pub rule ref_expr { ident | simple_string }
pub rule call_expr { '<' ident <rep expr> '>' }
pub rule seq_expr { <rep expr> }
pub rule block_expr { '{' <opt seq_expr> <rep {'|' seq_expr} {} {'}'}> '}' }


verbatim r#########"

fn parse_raw_string(s: &str) -> Option<usize> {
    let quote_start = s.find('"').unwrap();
    let q_hashes = concat!('"', "######", "######", "######", "######", "######");
    let closing = &q_hashes[..quote_start];
    s[quote_start + 1..].find(closing).map(|i| i + quote_start + 1 + closing.len())
}

"#########

ast {
  node file {
    nodes_def nodes_def?
    tokenizer_def tokenizer_def?
    syn_rules syn_rule*
    verbatim_def verbatim_def?
    ast_def ast_def?
  }

  node nodes_def { }

  node tokenizer_def {
    lex_rules lex_rule*
  }

  node lex_rule {
    node_type IDENT.text
  }

  node syn_rule {
    name IDENT.text
    block_expr block_expr
  }

  node verbatim_def { }

  node ast_def {
    ast_nodes ast_node_def*
    ast_classes ast_class_def*
  }

  node ast_node_def {
    name IDENT.text
    methods method_def*
  }

  node ast_class_def {
  }

  node method_def {
    name IDENT.text
  }

  node ref_expr {  }

  node call_expr {
    fn_name IDENT.text
    args expr*
  }

  node seq_expr {
    parts expr*
  }

  node block_expr {
    alts seq_expr*
  }

  class expr {
    ref_expr call_expr seq_expr block_expr
  }
}
