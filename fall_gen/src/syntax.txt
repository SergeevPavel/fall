nodes {
  kw_nodes
  kw_tokenizer
  kw_rule
  kw_verbatim

  eq pipe star
  lbrace rbrace
  langle rangle
  lparen rparen
  ident
  simple_string
  hash_string

  file
  string
  verbatim_def
  nodes_def
  tokenizer_def
  lex_rule
  syn_rule
  alt
  part
}

tokenizer {
    eq '='
    pipe '|'
    star '*'
    lbrace '{'
    rbrace '}'
    langle '<'
    rangle '>'
    lparen '('
    rparen ')'
    kw_nodes 'nodes'
    kw_tokenizer 'tokenizer'
    kw_rule 'rule'
    kw_verbatim 'verbatim'

    whitespace r"\s+"
    simple_string r#"'([^'\\]|\\.)*'"#
    hash_string r"r#*" 'parse_raw_string'
    ident r"\w+"
}

rule file { nodes_def tokenizer_def <rep syn_rule> verbatim_def }

rule nodes_def {
  'nodes' <commit> '{' <rep ident> '}'
}

rule tokenizer_def {
  'tokenizer' <commit> '{' <rep lex_rule> '}'
}

rule lex_rule { ident <commit> string <opt string> }

rule syn_rule {
  'rule' <commit> ident '{' syn_rule_body '}'
}
rule syn_rule_body { <opt alt> <rep '|' alt> }

rule alt { <rep part> }

rule part { ident | simple_string | '<' ident syn_rule_body '>' }

rule string { simple_string | hash_string }

rule verbatim_def { 'verbatim' hash_string }

verbatim r#########"

fn parse_raw_string(s: &str) -> Option<usize> {
    let quote_start = s.find('"').unwrap();
    let q_hashes = concat!('"', "######", "######", "######", "######", "######");
    let closing = &q_hashes[..quote_start];
    s[quote_start + 1..].find(closing).map(|i| i + quote_start + 1 + closing.len())
}

"#########
